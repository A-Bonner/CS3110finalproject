{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6583158-ea50-45e6-b23a-1874a1aab1b4",
   "metadata": {},
   "source": [
    "# CS 3110 Final Project\n",
    "### Adelaide Bonner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02b9d692-9825-4925-aac9-d8faa9a04d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "sets = pd.read_csv('https://github.com/A-Bonner/CS3110finalproject/raw/main/sets.csv')\n",
    "sets = sets.dropna()\n",
    "\n",
    "def pick_b_up(bs, col, epsilon, default):\n",
    "    last_result = 0\n",
    "\n",
    "    for b in bs:\n",
    "        clipped_sum = sets[col].clip(upper=b).sum()\n",
    "        result = laplace_mech(clipped_sum, sensitivity=b, epsilon=epsilon)\n",
    "        if result < last_result:\n",
    "            return b\n",
    "        else:\n",
    "            last_result = result\n",
    "    print('No good clipping parameter found')\n",
    "    return default\n",
    "\n",
    "def pick_b_down(bs, col, epsilon, default):\n",
    "    last_result = 0\n",
    "\n",
    "    for b in bs:\n",
    "        clipped_sum = sets[col].clip(lower=b).sum()\n",
    "        result = laplace_mech(clipped_sum, sensitivity=b, epsilon=epsilon)\n",
    "        if result < last_result:\n",
    "            return b\n",
    "        else:\n",
    "            last_result = result\n",
    "    print('No good clipping parameter found')\n",
    "    return default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b8c5a-6d97-4fea-bd36-06a40bfb770f",
   "metadata": {},
   "source": [
    "The goal of this project is to evaluate where and why clipping is and is not useful.  The first area of interest here is the 'num_parts' column, which has a wide range of part numbers from single digits to multiple thousands, but numbers in the high thousands are significantly rarer.  Presumably this makes the column an excellent candidate for clipping, as those higher numbers are outliers.  This is especially true because the dataset only includes sets released up to 2017, which is before LEGO began focusing more heavily on adult-aimed display sets that are typically very large (most of the largest sets ever released have come in the 2020s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c73f154b-3da1-484e-93bb-1b3dfc6c7c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 1300\n",
      "sum without clipping error: 1.9572273476833144\n",
      "mean without clipping error: 1.867990896828774\n",
      "sum with clipping at 1000 error: 10.099297838436641\n",
      "mean with clipping at 1000 error: 10.137382978098255\n",
      "sum with clipping at 3500 error: 0.4597840934190505\n",
      "mean with clipping at 3500 error: 0.46892539084907225\n",
      "sum with alogrithmically picked clipping parameter error: 6.249867453169542\n",
      "mean with alogrithmically picked clipping parameter error: 6.709461131267156\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "# first a mean and sum with no clipping\n",
    "# sum\n",
    "# using 6000 as sensitivity because no lego set with more than 6000 parts was released before 2017, but two sets came very close\n",
    "noisy_sum = laplace_mech(sets['num_parts'].sum(), 6000, epsilon)\n",
    "# mean\n",
    "noisy_count = laplace_mech(len(sets), 1, epsilon)\n",
    "noisy_mean = noisy_sum/noisy_count\n",
    "\n",
    "# then, those same queries with clipping for three different clipping parameters\n",
    "# the majority of sets can be assumed to be under 1000 parts, so try 1000\n",
    "noisy_sum_1000 = laplace_mech(sets['num_parts'].clip(upper=1000).sum(), 1000, epsilon)\n",
    "noisy_count_1000 = laplace_mech(len(sets['num_parts'].clip(upper=1000)), 1, epsilon)\n",
    "noisy_mean_1000 = noisy_sum_1000/noisy_count_1000\n",
    "\n",
    "# move higher, because there are still a sizable amount of sets with part numbers in the low thousands, try 3500\n",
    "noisy_sum_3500 = laplace_mech(sets['num_parts'].clip(upper=3500).sum(), 3500, epsilon)\n",
    "noisy_count_3500 = laplace_mech(len(sets['num_parts'].clip(upper=3500)), 1, epsilon)\n",
    "noisy_mean_3500 = noisy_sum_3500/noisy_count_3500\n",
    "\n",
    "# now use an algorithm to calculate the ideal clipping parameter\n",
    "bs = range(1000, 6000, 100)\n",
    "# using 6000 as the default value for the same reason it is used as the sensitivity\n",
    "b = pick_b_up(bs, 'num_parts', epsilon, 6000)\n",
    "print('b:', b)\n",
    "noisy_sum_b = laplace_mech(sets['num_parts'].clip(upper=b).sum(), b, epsilon)\n",
    "noisy_count_b = laplace_mech(len(sets['num_parts'].clip(upper=b)), 1, epsilon)\n",
    "noisy_mean_b = noisy_sum_b/noisy_count_b\n",
    "\n",
    "# lastly, a comparison of the error for the clipped and unclipped query results\n",
    "no_noise_sum = sets['num_parts'].sum()\n",
    "no_noise_mean = no_noise_sum/len(sets)\n",
    "\n",
    "#no clipping error\n",
    "no_clip_sum_err = pct_error(no_noise_sum, noisy_sum)\n",
    "no_clip_mean_err = pct_error(no_noise_mean, noisy_mean)\n",
    "print('sum without clipping error:', no_clip_sum_err)\n",
    "print('mean without clipping error:', no_clip_mean_err)\n",
    "\n",
    "#first param error\n",
    "sum_err_1000 = pct_error(no_noise_sum, noisy_sum_1000)\n",
    "mean_err_1000 = pct_error(no_noise_mean, noisy_mean_1000)\n",
    "print('sum with clipping at 1000 error:', sum_err_1000)\n",
    "print('mean with clipping at 1000 error:', mean_err_1000)\n",
    "\n",
    "#second param error\n",
    "sum_err_3500 = pct_error(no_noise_sum, noisy_sum_3500)\n",
    "mean_err_3500 = pct_error(no_noise_mean, noisy_mean_3500)\n",
    "print('sum with clipping at 3500 error:', sum_err_3500)\n",
    "print('mean with clipping at 3500 error:', mean_err_3500)\n",
    "\n",
    "#algorithmic param error\n",
    "sum_err_b = pct_error(no_noise_sum, noisy_sum_b)\n",
    "mean_err_b = pct_error(no_noise_mean, noisy_mean_b)\n",
    "print('sum with alogrithmically picked clipping parameter error:', sum_err_b)\n",
    "print('mean with alogrithmically picked clipping parameter error:', mean_err_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821f412-b31e-49d2-b355-510f67d8d829",
   "metadata": {},
   "source": [
    "Next, using the same queries as above, the usefulness of clipping for the release year of the sets will be tested.  Notably, however, the clipping parameters tested will be lower bounds, as LEGO's growth as a company has led to more sets being released year after year.  As well, none of the years can be considered outliers in the way that certain sets can be by piece count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79d358c0-caac-4130-b516-748d9eaad0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No good clipping parameter found\n",
      "b: 1934\n",
      "sum without clipping error: 0.004039798526354804\n",
      "mean without clipping error: 0.0012808980269958304\n",
      "sum with clipping at 1999 error: 0.2088161345656851\n",
      "mean with clipping at 1999 error: 0.32673782704745646\n",
      "sum with clipping at 1978 error: 0.03624971972923723\n",
      "mean with clipping at 1978 error: 0.13117562793270435\n",
      "sum with alogrithmically picked clipping parameter error: 0.01173188190999444\n",
      "mean with alogrithmically picked clipping parameter error: 0.08590374170968876\n"
     ]
    }
   ],
   "source": [
    "# no clipping\n",
    "# sum\n",
    "# 83 is used as sensitivity because the LEGO company has existed since 1934, and this dataset can therefore only have a range of 83 years\n",
    "noisy_sum = laplace_mech(sets['year'].sum(), 83, epsilon)\n",
    "# mean\n",
    "noisy_count = laplace_mech(len(sets), 1, epsilon)\n",
    "noisy_mean = noisy_sum/noisy_count\n",
    "\n",
    "# trying 1999, the year LEGO first licensed Star Wars\n",
    "noisy_sum_1999 = laplace_mech(sets['year'].clip(lower=1999).sum(), 18, epsilon)\n",
    "noisy_count_1999 = laplace_mech(len(sets['year'].clip(lower=1999)), 1, epsilon)\n",
    "noisy_mean_1999 = noisy_sum_1999/noisy_count_1999\n",
    "\n",
    "# trying 1978, the year the minifigure was introduced\n",
    "noisy_sum_1978 = laplace_mech(sets['year'].clip(lower=1978).sum(), 39, epsilon)\n",
    "noisy_count_1978 = laplace_mech(len(sets['year'].clip(lower=1978)), 1, epsilon)\n",
    "noisy_mean_1978 = noisy_sum_1978/noisy_count_1978\n",
    "\n",
    "# using an algorithm\n",
    "# 1934 is used because that is the year LEGO was created as a company\n",
    "bs = range(2017, 1934, 1)\n",
    "b = pick_b_down(bs, 'year', epsilon, 1934)\n",
    "print('b:', b)\n",
    "noisy_sum_b = laplace_mech(sets['year'].clip(lower=b).sum(), 2017-b, epsilon)\n",
    "noisy_count_b = laplace_mech(len(sets['year'].clip(lower=b)), 1, epsilon)\n",
    "noisy_mean_b = noisy_sum_b/noisy_count_b\n",
    "\n",
    "# comparing the results\n",
    "no_noise_sum = sets['year'].sum()\n",
    "no_noise_mean = no_noise_sum/len(sets)\n",
    "\n",
    "#no clipping error\n",
    "no_clip_sum_err = pct_error(no_noise_sum, noisy_sum)\n",
    "no_clip_mean_err = pct_error(no_noise_mean, noisy_mean)\n",
    "print('sum without clipping error:', no_clip_sum_err)\n",
    "print('mean without clipping error:', no_clip_mean_err)\n",
    "\n",
    "#first param error\n",
    "sum_err_1999 = pct_error(no_noise_sum, noisy_sum_1999)\n",
    "mean_err_1999 = pct_error(no_noise_mean, noisy_mean_1999)\n",
    "print('sum with clipping at 1999 error:', sum_err_1999)\n",
    "print('mean with clipping at 1999 error:', mean_err_1999)\n",
    "\n",
    "#second param error\n",
    "sum_err_1978 = pct_error(no_noise_sum, noisy_sum_1978)\n",
    "mean_err_1978 = pct_error(no_noise_mean, noisy_mean_1978)\n",
    "print('sum with clipping at 1978 error:', sum_err_1978)\n",
    "print('mean with clipping at 1978 error:', mean_err_1978)\n",
    "\n",
    "#algorithmic param error\n",
    "sum_err_b = pct_error(no_noise_sum, noisy_sum_b)\n",
    "mean_err_b = pct_error(no_noise_mean, noisy_mean_b)\n",
    "print('sum with alogrithmically picked clipping parameter error:', sum_err_b)\n",
    "print('mean with alogrithmically picked clipping parameter error:', mean_err_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
